---
title: Procademy, Q1, 08) Cache Memory - Basic
categories: ProcademyReview
tags: 
toc: true
toc_sticky: true
---

이 포스트는 프로카데미 (게임 서버 아카데미) 수업을 바탕으로 공부한 내용을 정리한 것입니다. 

# **1. Cache의 구조**

Cache는 D-RAM 보다 더 빠른 메모리(2023년 기준, 일반적으로 S-RAM)를 CPU 가까이에 붙여두고 자주 쓰는 데이터에 빠르게 접근할 수 있도록 만든 장치를 의미한다. 성능을 높이기 위해서는 Cache hit를 높여야 하는데, Cache 친화적 프로그래밍 하기 위해서는 시간적 지역성, 공간적 지역성 두가지를 고려해야 한다. 다차원 배열이 있을 때 메모리 구조상 연속적으로 접근할 수 있는 부분들 먼저 순서대로 접근하는 것이 이를 고려한 대표적인 기법으로 권장된다. 

공간적 지역성은 Cache 라인과 연관된다. 데이터 하나를 가져오기 위해서는 해당 데이터가 있는 라인을 통째로 긁어오게 되는데, 이때 64byte를 경계로 격자가 존재해서 이를 경계로 긁어온다. 따라서 붙어있는 데이터를 불러온다고 해도 Cache hit가 100%로 보장되는 것은 아니며, 그저 확률을 높여주는 것뿐이다.  

이때 변수가 속한 캐시 라인의 주소를 찾을 수도 있다. 어떤 변수 a의 캐시 라인 주소를 알고 싶다고 해보면,  64 단위로 경계가 세워져있기 때문에 a가 속한 지점의 앞쪽으로 64로 나누어 떨어지는 지점을 찾아갔을 때 거기가 a가 속한 캐쉬 라인의 시작 주소임을 알 수 있다. 즉, a - ( a % 64 ) 와 같이 나머지 값을 구해서 빼면 a의 위치를 구할 수 있고, 몫에 64를 곱하면 시작 위치를 구할 수 있다. 이를 비트 연산으로 표시하자면 00000 111 111가 64bit이므로 하위 6bit를 0으로 날리면 63-0까지 값이 모두 사라져 라인 주소 값만 남게 될 것이다. 따라서 1111 1111 000000 을 이용해 마스킹 하게 되면 그 결과 값이 a의 주소가 된다. 이를 코드로 표현하면 아래와 같다.

```c++
int main()
{
	int a = 0;
	unsigned int p = ((unsigned int )&a) & 0xffffffc0 ;
    printf(“0x%p 0x%p \n”, &a, p);
}
```
printf(“0x%p 0x%p \n”, &a, p) 이걸 출력해보면 Cache에서 a의 주소값과 a가 있는 Cache 라인의 시작 주소값을 알 수 있다. 하지만 주소를 명확하게 안다고 해도, 현재 대부분의 컴퓨터는 멀티 코어, 멀티 스레드 환경이기 때문에 Cache hit, Cache miss를 정확하게 통제할 순 없으며 Cache hit의 가능성이 높은 코드를 작성하는 것이 최선이다.

우리는 stack을 기반으로 돌기 때문에 전역 변수, 정적 변수와 같이 data영역에 있는 값을 사용할 경우 Cache Miss 가 날 확률이 높아지게 된다. 반면 Stack을 기반으로 사용할 경우 데이터들이 서로 밀접하게 붙어있을 가능성이 높아서 Cache Hit의 확률이 더 높아지게 된다.

<br/>

# **2. Cache 주소의 구성 요소**

캐시 구조는 동일한 아키텍처여도 다를 수 있으므로 직접 확인해보는 것이 좋다. (아키텍처는 기본적으로 논리 레지스터와 명령어의 통일만 보장한다.) 이 항목에서는 Cache 주소가 어떻게 구성되어 있는지, 그 구성이 왜 효율적인지 현 컴퓨터의 캐시 구조를 바탕으로 살펴볼 것이다. 일단 L1만 기준으로 두고 어떻게 하면 L1의 hit를 높일 수 있는지 계산해보고자 한다. 

작업 관리자 - 성능 에서 현 컴퓨터의 L1, L2, L3 캐시 용량을 볼 수 있다. 만약 L1 캐시가 256KB이라면, 이를 64로 나누면 4096 이므로 4096개의 Cache Line이 있다는 걸 알 수 있다. 하지만 한 코어에서 실제로 접근할 수 있는 Cache 공간의 수는 코어 개수, CPU 설계 방법 등에 따라 달라진다. Intel의 경우 코어 별로 L1 Cache, L2 Cache는 따로 갖고, L3 Cache는 모든 코어가 공유한다. 반면 ARM은 L3 Cache도 공유하지 않고 코어별로 따로 사용한다. 따라서 듀얼 코어만 되어도 한 코어가 256KB를 모두 갖는 게 아니라 128KB씩 나누어 쓰게 된다. 그리고 그 안에서도 또 데이터 캐시와 명령어 캐시가 나누어진다. 보통은 절반으로 나누지만 이것도 CPU 구조 따라서 나누는 비율이 달라질 수 있기 때문에 정확히 알고 싶다면 스펙을 확인해야 한다. CPU-Z를 통해 확인해봤을 때, 이 컴퓨터의 경우 코어 하나당 L1 캐쉬가 4*32KB 만큼 잡혀있었으며 이를 계산해보면 코어 하나당 사용할 수 있는 Cache Line의 수는 512개인 셈이 된다.

mov로 변수에 데이터를 저장하면 마스킹 기법으로 512개의 Cache Line 중 어디에 속해있는지 찾을 수 있다. 이는 Hash Table과 유사한 원리로, Key를 입력하면 Index가 계산되는 것을 활용하여 구현하는데 이러한 방법을 Direct Mapping이라고 한다. 주소를 Key로 삼아서, 주소를 바탕으로 Cache에 대한 인덱스가 무엇인지 알려주는 것이다. Hash 충돌을 막기 위해 Cache Memory에서는 기본적으로 메모리 주소의 일정 구간을 대놓고 Index로 쓰는 방식을 사용한다. 예를 들어 512개의 Cache Line이 있을 때 512는 2진수로 하면 0010 0000 0000이므로 각 라인 위치를 나타내기 위해 9bit가, 캐시 라인 내부 위치를 표현하기 위해 6bit가 필요할 것이다. 이때 Cache Line은 64byte의 공간이므로 하위 0-63는 캐시 라인의 내부 주소로 쓸 수 있게 된다. 

```
0x00100001
0x00100008
0x00100020
```

예를 들자면 위 예시들은 다 같은 캐시 라인에 있는 셈이다. 즉 하위 6bit는 offset, 캐시 라인 안에서의 상세 위치가 된다. 그러므로 32bit 중 상위 26bit로 Cache Line을 찾아야 된다. stack도 heap도 비슷한 주소 주변을 왔다갔다 하기 때문에 주소 체계에서 아래쪽을 잡는 게 좋다. 그래야 최대한 index가 다르게 나오도록 표현할 수 있게 된다. 상위 bit를 쓰게 되면 아래쪽 index끼리 충돌 날 가능성이 높아지는 반면 하위 bit를 쓰게 되면 index가 최대한 분포가 되기 때문에 충돌 날 가능성이 낮아지기 때문이다. Cache hit를 높이기 위해 공간적으로 근접한 것을 사용하는 것과 유사한 원리이다. 따라서 이 컴퓨터 환경에서 주소는 아래와 같이 표현되는 것이 효율적일 것이다.

| 0000 000 | 0 0000 0000  | 0000 00 |
|   Tag    |     Index    |  Offset |

이때, Tag는 내가 찾은 인덱스가 정확한지 확인하기 위해 쓰인다. 캐시 라인 별로 Tag가 존재해서, Index로 찾아간 이후에 Tag를 비교하여 다시 한번 확인하는 것이다. 그러나 이렇게 구성할 경우 하나의 index에 하나의 tag밖에 들어가지 못하는 셈이 된다. 따라서 이를 보안하기 위해 way라는 방법이 들어간다. 하나의 index에 라인을 두개 넣으면 2way, 네개 넣으면 4way, 여섯개 넣으면 6way가 된다. 최근에 메모리 사용의 대역이 커지면서, 점점 인덱스를 줄이고 way를 넓히는 방향으로 가고 있다. 인덱스를 줄이더라도 더 넓은 대역에 대한 충돌을 낮출 수 있다면 이게 더 효율적일 것이다 라는 판단 하에 way를 8개로 늘려서 충돌의 가능성을 1/8로 낮추는 것이다. 이처럼 한 Index에 캐시 라인이 여러개 들어있고 이게 tag로 구분 된다면 더 효율적으로 사용할 수 있게 될 것이다. 단 이렇게 되면 탐색에도 시간이 필요하게 되므로 way를 너무 늘리는 것은 좋지 않다.

만약 L1를 8 way로 사용하고 있다면 기존 계산에서 Index에 쓸 수 있는 비트 수가 그만큼 줄어들 것이다. 이 상황에서는 512 / 8을 하면 index는 64개가 된다. 따라서 현 컴퓨터의 캐시는 64개 index로 이루어진 캐시 테이블을 갖고 있고 각 캐시 테이블에는 8개의 캐시 라인이 들어갈 것이다. 이를 반영하면 주소는 아래와 같이 수정된다. 

| 0000 0000 00 |  00 0000 | 0000 00 |
|      Tag     |   Index  |  Offset |


이러한 주소 체계를 효율적인 프로그래밍에 직접 활용하기는 어렵다. 그러나 이에 대해 알고 있으면 관련된 문제가 생겼을 때 유용하게 사용할 수 있다. 동일한 로직인데도 어떻게 메모리에 접근하느냐에 따라 성능이 떨어지는 상황이 간혹 발생한다. 이때 논리적으로 문제가 없는데도 동일한 구간에서 성능 저하가 계속 일어난다면 Index 충돌을 유도하는 자료구조, 알고리즘을 사용하고 있지는 않은가 살펴볼 수 있다. 동적할당을 하는데 그게 특정한 크기면 계속 Index 충돌이 나는 등, Index 충돌로 인한 Cache miss가 반복되는 상황이 간혹 존재하기 때문에, Cache의 구조 및 Cache miss가 일어나는 요인에 대해 숙지하는 것이 좋다. 

<br/>

# **3. Cache의 주소 체계**

캐시 메모리 안에서는 주소를 바탕으로 위치를 정한다. 일반적으로 주소는 가상 주소와 물리 주소가 있는데, 우리가 보는 메모리 주소는 대부분 가상 주소에 속하며 우리는 물리 주소를 직접볼 수 없다. 따라서 내 컴퓨터에서 1000번지가 다른 컴퓨터에서 1000번지와 다를 수 있는 것이다. 이때 가상 주소를 정하는 것은 운영체제가 아니다. CPU, MMU가 프로세스마다 가상 주소 테이블을 만들고, 그 가상 주소의 번지를 사용하면 그 규칙을 운영체제가 따라주는 것이다. 이때 1:1로 맵핑하면 충분히 많은 주소를 사용할 수 없기 때문에 페이징 기법을 사용하게 된다. 가상 주소 안에서의 4K 영역은 페이지, 물리주소에서 4K 영역은 프레임이라고 부른다. 이러한 구조도 CPU가 설계한 것이지 Windows가 설계한 게 아니다.

이때 캐시 메모리는 가상 주소와 물리 주소의 체계를 섞어서 사용한다. 캐시에 가상 주소를 쓰면 index를 고루 분포시킬 수 있어서 그대로 주소로 사용할 수 있는 반면 물리 주소를 쓸 경우 고유한 주소가 되기 때문에 index가 넓게 퍼져있을 거라고 보장받을 수 없게 된다. 반면 가상 주소를 쓰면 스위칭 과정에서 해당 주소가 어떤 프로세스의 몇 번지인지에 대한 추가 정보가 필요한데, 물리 주소를 쓰면 이런 과정을 생략할 수 있어서 효율적이다. 섞어서 사용하면 각 방법의 장점을 모두 얻을 수 있다. 

| 1111 1111 11 |  00 0000 | 0000 00 |
|      Tag     |   Index  |  Offset |

이를 16진수로 바꾸면 0x ffff f000 와 같이 된다. 이 중 하위 12bit (Index + Offset)은 가상 주소, 그 외 상위 Tag 부분은 물리 주소를 사용한다.

가상 주소의 비트를 보면 상위 몇비트는 페이지(프레임)을 나타내고 하위 12bit는 Page Offset이 된다. 이때 하위 6bit의 오프셋은 페이지에서와 프레임에서의 값이 일치한다. 따라서 가상 메모리 Table에서도 이 Offset에 대한 정보는 포함하지 않는다. 그리고 상위 Tag에 해당하는 부분은 물리주소를 구분하는 데에 사용한다. 이렇게 되면 Index에서는 가상 주소를 사용하기 때문에 Cache Line Index 를 Cache hit를 늘리는 방향으로 구성할 수 있으며, Tag는 물리 주소를 사용하기 때문에 프로세스가 달라져도 고유한 값을 가질 수 있게 된다. 
 
간혹 몇 서적이나 인터넷 정보를 보면 물리주소와 가상주소를 섞는 bit에 대해 정확하지 않게 표기하는 경우가 있다. 특히 10bit / 10bit씩 나눠서 표기하는 바람에 index 상위 2bit는 물리로 하위 4bit는 가상으로 쓰이게끔 표기하는 경우가 많은데 이렇게 할 경우 가상 주소를 사용하는 이유가 없어진다. 이는 잘못된 예시니 걸러서 보면 된다. 

현재 대부분의 컴퓨터는 캐시 메모리의 크기에 상관 없이 index를 6bit, offset를 6bit로 사용하고 있다. 캐시 메모리를 늘려도 보통 way 수를 늘리는 방향으로 사용한다.

<br/>

# **4. 캐시의 Write 정책**

만약 mov a 1을 한다면 이를 L1, L2, L3, RAM 까지 실행할까 아니면 가장 가까이 있는 L1 에만 반영할까? 혹은 확인하지 않고 그냥 명령을 던지기만 할까? 이런 것을 결정하는 정책을 두고 Write 정책이라고 한다.

실제로는 Write 정책에는 WriteThrow, WriteBack 두가지 정책이 있다. L1까지만 쓰고 확인하는 것은 WriteBack, 그냥 명령을 던지기만 하는 것은 WriteThrow라고 한다. Write Throw는 코드 영역에서 사용하고, Write Back은 데이터 영역에서 사용된다. 코드 영역은 변화가 많지 않기 때문에 굳이 Back으로 확인해줄 필요가 없다. 반면 데이터 영역은 변화가 계속 일어나기 때문에 Write Back을 이용하여 확실하게 확인을 해주어야 한다. Wirte Back을 하게 되면 L1 캐시의 값만 우선 바꾸고, 이를 확인한 뒤, L1 캐시에서 해당 영역을 지울 때 아래 단계로 내리게 된다.

(+ 참고로 L1에서 일단 L2로만 내리고, L2에서 해당 영역을 지울 때 L3로 내리는 것인지, 혹은 바로 L2, L3, RAM까지 내리는지에 대해 제조사들이 아직 명확하게 밝히지 않고 있다. 대부분의 자료가 “L1만 바꾼다, 그리고 이후에 RAM으로 반영된다” 이 내용만 담고 있지 그 사이의 과정에 대해 다루고 있지 않다.)

이때, L1의 값을 변경하였다면 값이 변경되었으니 해당 값에 대해 나중에 반영하라는 내용을 저장하고 있어야 한다. Intel의 경우 L3를 일종의 통신 매개체로 쓰기 때문에, L1에 있는 값을 변경하면 L3(공용 캐시)에 "이 값이 변경되었으니 나중에 반영하라"고 명령을 내린다. 반면 AMD는 L3를 공용 캐시로 쓰지 않기 때문에 공유 인터페이스에 명령을 내린다. 따라서 WriteBack의 과정은 캐시 메모리 구조 및 운영체제에 따라 달라질 수 있다.

<br/>

# **5. 질문**

**Q.** 내부 레지스터, 버퍼, 캐시 구조 등은 visual studio에서 확인할 수 없나요?

2023년 기준 visual studio에서는 논리 레지스터와 메인 메모리만 확인 가능합니다.

**Q.** 캐시 라인을 키우지 않고 way만 늘리는 이유는?

64bit 체제에서 64byte가 적절해서 그런 걸로 추측합니다. 만약 언젠가 128bit 운영체제가 나온다면 달라질 수도 있겠습니다만 이는 그냥 소비자의 추측입니다.

**Q.** 그럼 32bit에서는 8byte 자료형이 8의 경계에 있든 4의 경계에 있든 성능상 차이는 없는 건가요? 그렇다면 왜 8byte로 패딩을 하나요

어차피 32bit에서는 두 번 왔다갔다 해야하므로 성능상 차이는 없습니다. 아마도 통일성을 위한 것으로 추정됩니다. 추후 배우겠지만 align 키워드를 쓰면 stack에서도 경계에 세울 수 있게 됩니다.

**Q.** way를 늘리면 충돌을 줄일 수 있지만 대역폭이 커진다고 하셨는데 무슨 의미인가요?

인덱스의 폭이 넓어진다고 하면 상위 비트만 바뀌는 메모리 번지를 사용했을 때 충돌이 늘어납니다. 비슷한 메모리 번지만 사용하는 구조일 때는 그래도 상관이 없겠지만, 메모리 번지를 넓게 사용한다면 이랬을 때 충돌 날 확률이 높을 것입니다. 반면 같은 용량에서 way를 늘리면 인덱스로 쓸 수 있는 범위가 줄어듭니다. 따라서 비교 연산에 의한 성능 저하를 감수하고서라도 넓은 메모리 번지를 사용하자는 의미에서 way가 나온 것입니다.

